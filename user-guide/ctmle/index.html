<!DOCTYPE html>
<html lang="en">
    <head>
        <meta charset="utf-8">
        <meta http-equiv="X-UA-Compatible" content="IE=edge">
        <meta name="viewport" content="width=device-width, initial-scale=1.0">
        
        
        <link rel="canonical" href="https://lendle.github.io/TargetedLearning.jl/user-guide/ctmle/">
        <link rel="shortcut icon" href="../../img/favicon.ico">

	<title>(C)TMLE - TargetedLearning.jl</title>

        <link href="../../css/bootstrap-custom.min.css" rel="stylesheet">
        <link href="../../css/font-awesome-4.0.3.css" rel="stylesheet">
        <link rel="stylesheet" href="../../css/highlight.css">
        <link href="../../css/base.css" rel="stylesheet">

        <!-- HTML5 shim and Respond.js IE8 support of HTML5 elements and media queries -->
        <!--[if lt IE 9]>
            <script src="https://oss.maxcdn.com/libs/html5shiv/3.7.0/html5shiv.js"></script>
            <script src="https://oss.maxcdn.com/libs/respond.js/1.3.0/respond.min.js"></script>
        <![endif]-->

        
        <script>
            (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
            (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
            m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
            })(window,document,'script','//www.google-analytics.com/analytics.js','ga');

            ga('create', 'UA-62410095-1', 'https://lendle.github.io/TargetedLearning.jl/');
            ga('send', 'pageview');
        </script>
        
    </head>

    <body>

        <div class="navbar navbar-default navbar-fixed-top" role="navigation">
    <div class="container">

        <!-- Collapsed navigation -->
        <div class="navbar-header">
            <!-- Expander button -->
            <button type="button" class="navbar-toggle" data-toggle="collapse" data-target=".navbar-collapse">
                <span class="sr-only">Toggle navigation</span>
                <span class="icon-bar"></span>
                <span class="icon-bar"></span>
                <span class="icon-bar"></span>
            </button>

            <!-- Main title -->
            <a class="navbar-brand" href="../..">TargetedLearning.jl</a>
        </div>

        <!-- Expanded navigation -->
        <div class="navbar-collapse collapse">
            <!-- Main navigation -->
            <ul class="nav navbar-nav">
            
            
                <li >
                    <a href="../..">Home</a>
                </li>
            
            
            
                <li class="dropdown active">
                    <a href="#" class="dropdown-toggle" data-toggle="dropdown">User Guide <b class="caret"></b></a>
                    <ul class="dropdown-menu">
                    
                        <li >
                            <a href="../julia/">Julia</a>
                        </li>
                    
                        <li >
                            <a href="../estimation/">The estimation problem</a>
                        </li>
                    
                        <li class="active">
                            <a href="./">(C)TMLE</a>
                        </li>
                    
                        <li >
                            <a href="../influencecurves/">Automatic influence curves</a>
                        </li>
                    
                        <li >
                            <a href="../lalonde_example/">Lalonde Example</a>
                        </li>
                    
                    </ul>
                </li>
            
            
            
                <li class="dropdown">
                    <a href="#" class="dropdown-toggle" data-toggle="dropdown">API <b class="caret"></b></a>
                    <ul class="dropdown-menu">
                    
                        <li >
                            <a href="../../api/tmles/">TMLEs</a>
                        </li>
                    
                        <li >
                            <a href="../../api/ctmles/">CTMLEs</a>
                        </li>
                    
                        <li >
                            <a href="../../api/lreg/">LReg</a>
                        </li>
                    
                        <li >
                            <a href="../../api/parameters/">Parameters</a>
                        </li>
                    
                    </ul>
                </li>
            
            
            
                <li class="dropdown">
                    <a href="#" class="dropdown-toggle" data-toggle="dropdown">About <b class="caret"></b></a>
                    <ul class="dropdown-menu">
                    
                        <li >
                            <a href="../../about/license/">License</a>
                        </li>
                    
                    </ul>
                </li>
            
            
            </ul>

            <!-- Search, Navigation and Repo links -->
            <ul class="nav navbar-nav navbar-right">
                
                <li >
                    <a rel="next" href="../estimation/">
                        <i class="fa fa-arrow-left"></i> Previous
                    </a>
                </li>
                <li >
                    <a rel="prev" href="../influencecurves/">
                        Next <i class="fa fa-arrow-right"></i>
                    </a>
                </li>
                
                <li>
                    <a href="https://github.com/lendle/TargetedLearning.jl/">
                        
                            <i class="fa fa-github"></i>
                        
                        GitHub
                    </a>
                </li>
                
            </ul>
        </div>
    </div>
</div>

        <div class="container">
            <div class="col-md-3"><div class="bs-sidebar hidden-print affix well" role="complementary">
    <ul class="nav bs-sidenav">
    
        <li class="main active"><a href="#notation">Notation</a></li>
        
    
        <li class="main "><a href="#tmle">TMLE</a></li>
        
            <li><a href="#fluctuating-barq_n">Fluctuating $\bar{Q}_n$</a></li>
        
            <li><a href="#example-implementation">Example implementation</a></li>
        
            <li><a href="#example-using-targetedlearningjl">Example using TargetedLearning.jl</a></li>
        
    
        <li class="main "><a href="#ctmle">CTMLE</a></li>
        
            <li><a href="#using-ctmle-in-the-targetedlearningjl-package">Using ctmle in the TargetedLearning.jl package</a></li>
        
            <li><a href="#search-strategies">Search strategies</a></li>
        
            <li><a href="#cross-validation">Cross validation</a></li>
        
            <li><a href="#example-using-targetedlearningjl_1">Example using TargetedLearning.jl</a></li>
        
    
    </ul>
</div></div>
            <div class="col-md-9" role="main">

<script type="text/javascript"
  src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
  MathJax.Hub.Config({
    tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]},
              processEnvironments: true
             }
  );
</script>

<h1 id="notation">Notation</h1>
<ul>
<li>$O=(W, A, Y)$: Observed data structure<ul>
<li>$W$: real valued vector of baseline covariates</li>
<li>$A$: binary indicator of treatment (or missingness)</li>
<li>$Y$: scalar outcome, binary or bounded by $0$ and $1$.</li>
</ul>
</li>
<li>$O_i = (W_i, A_i, Y_i)$: $i$th observation for $i = 1, \ldots, n$.</li>
<li>$\mathcal{M}$: statistical model</li>
<li>$P\in \mathcal{M}$: a distribution of observed data $O$</li>
<li>$\Psi$: A statistical parameter mapping from $\mathcal{M}$ to $\mathbb{R}^k$.</li>
<li>$\psi$: $\Psi(P)$ for some $P$</li>
<li>$E_P$: expectation w.r.t. distribution $P$.</li>
<li>$\bar{Q}(a, w)$: $E_P(Y\mid A=a, W=w)$ for some $P$</li>
<li>$Q_W(w)$: $P(W=w)$, the marginal distribution of $W$.</li>
<li>$Q$: $(\bar{Q}, Q_W)$</li>
<li>$g(a \mid w)$ : $P(A=a\mid W=w)$</li>
<li>Subscript $0$ denotes some quantity for the true distribution, <em>e.g.</em> $\bar{Q}_0$ is $\bar{Q}$ where the expectation is taken w.r.t. the trued istribution $P_0$.</li>
<li>Subscript $n$ denotes an estimate based on $n$ observations, <em>e.g.</em> $g_n$ is an estimate of $g_0$.</li>
</ul>
<h1 id="tmle">TMLE</h1>
<p>TMLE is a general framework for constructing regular, asymptotically linear plug-in estimators. Details about how to use the TMLE framework to construct an estimator can be found in the <a href="https://www.springer.com/statistics/statistical+theory+and+methods/book/978-1-4419-9781-4">Targeted Learning</a> book by van der Laan and Rose, or <a href="http://scholar.google.com/scholar?q=targeted+estimation+tmle">articles</a> on TMLE.</p>
<p>Here we'll walk through an example of how to actually implement a TMLE for the average treatment effect (ATE) and skip over the details of the derivation.</p>
<p>Using <a href="../estimation/#counterfactuals-and-causal-parameters">counterfactuals</a>, we define the ATE as $E_0(Y_1 - Y_0)$. Under causal assumptions, we can write this causal parameter as a parameter of the distribution of the observed data:
\begin{align}
\psi_0 = \Psi(P_0) =&amp; E_0(E_0(Y\mid A = 1, W) - E_0(Y\mid A=0, W)) \\
=&amp; E_{Q_{W0}}(\bar{Q}_0(1, W) - \bar{Q}_0(0, W)).
\end{align}
The parameter is computed by first taking the difference $\bar{Q}_0(1, W) - \bar{Q}_0(0, W)$, then averaging over $W$, so $\Psi$ only depends on $P$ through $Q=(\bar{Q}, Q_W)$. Recognizing the abuse of notation, we sometimes write $\Psi(\bar{Q}, Q_W)$. $g_0(a \mid w)$, sometimes called the propensity score, is a nuisance parameter.</p>
<p>A <strong>plug-in</strong> estimator is one that first estimates (relevant parts of) the distribution $P_0$ and then plugs it in to the parameter mapping. In our case, that's $\bar{Q}_0$ and $Q _ {W0}$.
A <strong>TMLE</strong> is a plug in estimator constructed by first finding an initial estimate of $Q_0$, and then updating those estimates using estimates of nuisance parameters ($g _ 0$ here), then computing a plug-in estimate based on the updated estimate of $Q$.
The update is done by fluctuating the initial estimator in such a way that the final plug in estimator is locally efficient and doubly robust.
It turns out in this example, that only the initial estimator of $\bar Q _ 0$ (and not $Q _ {Wn})$ requires a fluctuation.</p>
<p>$\bar{Q} _ 0$ is the conditional mean of $Y$, and can be estimated with regression techniques, <em>e.g.</em> logistic regression or something more flexible.  $Q _ {W0}$ is just a marginal distribution, and the easiest way to estimate that is with empirical distribution, so we'll use that for for $Q _ {Wn}$.
The fluctuation relies on an estimate of $g_0$, which is another conditional mean.</p>
<!---

Given estimates $\bar{Q} _ n$ and $Q _ {Wn}$ a simple plug in estimator for $\psi_0$ is be computed as
$$
\Psi(\bar{Q}_n, Q _ {Wn}) = \frac 1n \sum _ {i=1}^n (\bar{Q}_n(1, W_i) - \bar{Q}_n(0, W_i)).
$$

In general, an estimation method may not always result in an estimate that falls in the parameter space, particularly in small samples, even if that method is consistent or even efficient.
Plug-in estimators are one way to guarantee that estimates are always in the parameter space. For example, we know that the ATE must be between $-1$ and $1$, because $Y \in \[0, 1\]$. The plug-in estimator $\Psi(\bar{Q}_n, Q _ {Wn})$ will always be in $\[0, 1\]$, provided $\bar{Q}_n(a, w)$ is yields estimates in $\[0, 1\]$.

Plug-in estimators, however, are not efficient in general. TMLE constructs an efficient plug-in estimator by taking an initial estimate of $Q$, and updates it to $Q_n^*$ in such a way that the so-called efficient influence curve (EIC) equation is solved. For this particular example, only $\bar{Q}_n$ needs to be updated, and not $Q _ {Wn}$.  This update is also called a fluctuation.
 -->

<h2 id="fluctuating-barq_n">Fluctuating $\bar{Q}_n$</h2>
<p>Fluctuating the initial estimate $\bar{Q} _ n$ amounts to regressing $Y$ on a particular covariate using $\bar{Q} _ n$ as an offset.
In particular, we use the model
<!-- We have a choice of two fluctuation procedures: an *unweighted fluctuation* or a *weighted fluctuation*.
To describe them, we first we define a parametric submodel through the initial $\bar{Q}_n$, $\\{\bar{Q}_n(\epsilon) : \epsilon \in \mathbb{R} \\}$ using an estimate of $g_0$ such that $\bar{Q}_n(\epsilon=0) = \bar{Q}_n$. For the ATE, we will use
 -->
 $$
\mbox{logit} \bar{Q}_n(\epsilon)(a, w) = \mbox{logit} \bar{Q}_n(a, w) + h(a, w) \epsilon
$$
where $\mbox{logit}(x) = \log(x)/\log(1-x)$ and $h$ is a known function.
We also define a loss function for $\bar{Q}_n(\epsilon)$:
$$
\ell(\bar{Q}_n(\epsilon))(O) = - b(A, W) [Y \log(\bar{Q}_n(\epsilon)(A, W)) - (1-Y) \log (1-\bar{Q}_n(\epsilon)(A, W))],
$$
where $b$ is a known function.</p>
<p>If $Y$ is binary, you might recognize this as a logistic regression model for the conditional mean of $Y$ on the covariate $h(A,W)$ with $\mbox{logit} \bar{Q}_n(A, W)$ as a fixed offset and weights $b(A,W)$.
When $Y$ is not binary but bounded between $0$ and $1$, this is still a valid loss function for the conditional mean of $Y$.</p>
<p>In TargetedLearning.jl, there are two types of fluctuations supported, <em>unweighted</em> and <em>weighted</em>, which define the covariate and weight functions
$h$ and $b$.</p>
<ul>
<li><em>Unweighted fluctuation</em>:
\begin{gather}
h(a,w) = \frac{2a-1}{g_n(a\mid w)},  &amp; &amp;
b(a,w) = 1
\end{gather}</li>
<li><em>Weighted fluctuation</em>:
\begin{gather}
h(a,w) = 2a-1, &amp; &amp;
b(a,w) = \frac{1}{g_n(a\mid w)}
\end{gather}
<!--
Both are chosen such that
$$
h(a,w)b(a,w) = \frac{2a-1}{g_n(a\mid w)}.
This means that the score equation for $\epsilon$ in our quasi-logistic regression model
$$
\frac 1n \sum _ {i=1}^n \frac{2A-1}{g_n(A\mid W)} (Y - \bar{Q}_n(A,W)(\epsilon))
$$
is solved at $\epsilon_n$.
$$ --></li>
</ul>
<p>Once this choice is made, we compute an estimate of $\epsilon$ by minimizing the empirical mean of $\ell$:
$$
\epsilon_n = \arg\min _ {\epsilon} \sum _ {i =1} ^n \ell(\bar{Q}_n(\epsilon))(O_i).
$$
This amounts to fitting a (quasi-)logistic regression model with an offset and possibly with a weight.</p>
<p>Finally we set $\bar{Q}_ n ^* = \bar{Q}_n(\epsilon_n)$ and compute the final estimate of $\psi_0$ as $\Psi(\bar{Q} _ n ^*, Q _ {Qn})$:
$$
\Psi(\bar{Q} _ n^*, Q _ {Wn}) = \frac 1n \sum _ {i=1}^n (\bar{Q}_n^*(1, W_i) - \bar{Q}_n^*(0, W_i)).
$$</p>
<p>This final TMLE is a plug-in estimator by definition, and it is also doubly robust and locally efficient, meaning that if either the initial $\bar{Q}_n$ or $g_n$ estimate $\bar{Q}_0$ or $g_0$ consistently respectively, then $\Psi(Q_n^*)$ is consistent, and if both $\bar{Q}_0$ and $g_0$ are consistent, then $\Psi(Q_n^*)$ is efficient.</p>
<h2 id="example-implementation">Example implementation</h2>
<p>The math makes things a lot more complicated than they really are. Here's a simple implementation using the unweighted fluctuation:</p>
<pre><code class="julia"># input:
# logitQnA1, logitQnA0: vectors containing initial estimates \logit(\bar{Q}_n(a, W)) for a = 1 and 0, respectively
# gn1: a vector of containing estimates g_n(1 \mid W)
# A: binary treatment vector
# Y: outcome vector
# lreg is a simple wrapper function for logistic regression using the GLMNet.jl package

function simpletmle(logitQnA1, logitQnA0, gn1, A, Y)
    logitQnAA = ifelse(A.==1, logitQnA1, logitQnA0)
    gnA = ifelse(A .==1, gn1, 1 .- gn1)

    Qfit = lreg((2A .-1)./(gnA), Y, offset = logitQnAA)

    Qn⋆A1 = predict(Qfit, 1./gn1, offset = logitQnA1)
    Qn⋆A0 = predict(Qfit, -1./(1 .- gn1), offset = logitQnA0)

    return mean(Qn⋆A1 .- Qn⋆A0)
end
</code></pre>

<h2 id="example-using-targetedlearningjl">Example using TargetedLearning.jl</h2>
<p>Continuing the <a href="../julia/#example">example</a> from the intro to Julia, we now demonstrate how to use TMLE to estimate the ATE from the Lalonde data set.</p>
<p><a href="http://nbviewer.ipython.org/url/lendle.github.io/TargetedLearning.jl/user-guide/lalonde_example.ipynb#TMLE-for-the-average-treatment-effect">The example can be found here.</a></p>
<h1 id="ctmle">CTMLE</h1>
<p>A TMLE takes user provided estimates of $\bar{Q}_0$ and $g_0$ and is doubly robust, meaning that the estimate of $\psi_0$ is consistent if either estimate of $\bar{Q}_0$ and $g_0$ is consistent. Estimates of $g_0$ typically try to predict treatment given $W$ as well as possible. In finite samples and particularly when $W$ is high dimensional, this is not always helpful when the goal is to estimate $\psi_0$, and can lead to higher variance without satisfactorally reducing bias. Instead of taking a user specified estimate of $g_0$, CTMLE tries to build an estimate of $g_0$ adaptively in a way that is aimed at reducing bias in the final estimate of $\psi_0$. For technical details, see Collaborative Double Robust Targeted Maximum Likelihood Estimation by Mark van der Laan and Susan Gruber<sup id="fnref:ctmle_paper"><a class="footnote-ref" href="#fn:ctmle_paper" rel="footnote">1</a></sup>.</p>
<p>Recall the  <a href="../estimation/#assumptions">randomization assumption</a>, which requires that all potential confounders, (baseline covariates which can affect both treatment and outcome,) are included in $W$. $W$ can also include covariates that are related to either the outcome or treatment but not both, so are not confounders. CTMLE works by taking a sequence of estimators of $g_0$ which start out very simple, (an intercept only model, say,) and increase in complexity. An initial estimate of $\bar{Q}_0$ is then fluctuated with each of $g_0$ in the sequence. This process is stopped after some number of steps determined by a cross-validated loss for $\bar{Q}_0$.</p>
<p>CTMLE takes advantage of the so called collaborative double robustness property<sup id="fnref:ctmle_paper"><a class="footnote-ref" href="#fn:ctmle_paper" rel="footnote">1</a></sup> of the estimation problem.
In short, this means that a final CTMLE of $\psi_0$ can be consistent even if estimates of $\bar{Q}_0$ and $g_0$ are both not consistent, but "collaborate" to  adjust for all confounding.
The idea is that we want to choose a sequence of estimators for $g_0$ that adjust for the most important potential confounders in $W$ first. By important, we mean a covariate that helps reduce the bias in the final estimate of $\psi_0$ the most. How important a covariate is depends on how strongly it is related to both the treatment and the outcome, and also how well the initial estimate of $\bar{Q}_0$ adjusts for that covariate.</p>
<p>For example, suppose we have 3 covariates:</p>
<ul>
<li>$W_1$: Strongly related to both treatment and outcome, and initial estimate $\bar{Q}_n$ is already adjusting for it well.</li>
<li>$W_2$: Strongly related to both treatment and outcome, but initial estimate $\bar{Q}_n$ is not adjusting for it well. For example suppose $\bar{Q}_0$ depends on $W_2$ in some non-linear way but $\bar{Q}_n$ only adjusts for it linearly.</li>
<li>$W_3$: Strongly related to treatment but is unrelated to outcome.</li>
</ul>
<p>Ideally, we'd want to adjust for $W_2$ first, because it is a strong confounder that we're not already handling well, so it could potentially remove the most bias from an estimate of $\psi_0$.
Because $W_1$ is a confounder but $\bar{Q}_n$ is already doing a good job of adjusting for it, and $W_3$ is not a confounder, we may already have enough to estimate $\psi_0$ without bias thanks to collaborative double robustness.
Because $W_3$ is not a confounder, (it is an instrumental variable<sup id="fnref:iv"><a class="footnote-ref" href="#fn:iv" rel="footnote">2</a></sup> in this case,) the next best choice is $W_1$, so the next estimate of $g_0$ will adjust for $W_1$ and $W_2$. This process is continued until the number of steps chosen by cross-validation is reached.</p>
<p>In the implementation of CTMLE in TargetedLearning.jl, there are a number of hueristic <a href="#search-strategies">options</a> for how estimates of $g_0$ are chosen and ordered.</p>
<h2 id="using-ctmle-in-the-targetedlearningjl-package">Using <code>ctmle</code> in the TargetedLearning.jl package</h2>
<p>The <code>ctmle</code> function is called with required arguments</p>
<ul>
<li><code>logitQnA1</code> Vector of initial estimates $\mbox{logit}(\bar{Q}_n(1, W_i))$</li>
<li><code>logitQnA0</code> Vector of initial estimates $\mbox{logit}(\bar{Q}_n(0, W_i))$</li>
<li><code>W</code> Matrix of potential confounders to be used in estimation of $g_0$.</li>
<li><code>A</code> Vector of treatments, 0s or 1s</li>
<li><code>Y</code> Vector of outcomes in $[0, 1]$</li>
</ul>
<p>Anything can be used for initial estimates of $\bar{Q}_0$ from a simple GLM to something more data adaptive.
The initial estimate of $\bar{Q}_0$ can also depend on other covariates that are not included in the <code>W</code> argument if that makes sense in your problem.
Note that <code>ctmle</code> does not currently cross-validate the intitial estimate $\bar{Q}_n$, so it is important that the initial esitmate of $\bar{Q}_0$ is not overfit.</p>
<p>The target parameter is specified by the <code>param</code> keyword argument as described in <a href="../estimation/#statistical-target-parameters">the estimation problem</a>. Other options are described below.</p>
<h2 id="search-strategies">Search strategies</h2>
<p>Theorey requires that the sequence of estimators of $g_0$ increase in complexity up to some estimator which is consistent for $g_0$<sup id="fnref:ctmle_paper"><a class="footnote-ref" href="#fn:ctmle_paper" rel="footnote">1</a></sup>, but there are many ways to construct this sequence. TargetedLearning.jl uses logistic regression and adds covariates sequentially to each estimate of $g_0$. The package provides a variety of strategies for searching for the next covariate(s) to include in an estimate for $g_0$ which make different tradeoffs in statistical and computational performance.</p>
<p><strong>Forward stepwise</strong> strategies choose the next covariate to add to an estimator of $g_0$ by selecting the best (in terms of some criterion) covariate among those not already used. For $p$ covariates, this requires the criterion to be computed $O(p^2)$ times, which can be costly when $p$ is bigger than a dozen or so.</p>
<p>In the <code>ctmle</code> function, the forward stepwise strategy is specified by setting the keyword argument <code>searchstrategy=ForwardStepwise()</code>.  The criterion used for choosing the next covariate is the (quasi-)binomial loss function for $\bar{Q}_0$ after fluctuating the previous estimate of $\bar{Q}_0$ with the a estimate of $g_0$ including the new and previous covariates. This is essentially the same as the method described in van der Laan, M. &amp; Gruber, S. (2010)<sup id="fnref:ctmle_paper"><a class="footnote-ref" href="#fn:ctmle_paper" rel="footnote">1</a></sup>, except TargetedLearning.jl does not penalize the likelihood.</p>
<p><strong>Pre-ordered</strong> strategies compute some criterion once up front, and order covariates by that criterion. This is less agressive than a forward step wise procedure, but it only requires the criterion to be computed $p$ times for $p$ covariates.</p>
<p>A pre-ordered strategy is specified by setting <code>searchstrategy=PreOrdered(ordering)</code>, where <code>ordering</code> defines the criterion to use for ordering covariates.
Choices of selection criterion are:</p>
<ul>
<li><code>LogisticOrdering()</code> - Log(Quasi-)binomial loss for an iniital estimate of $\bar{Q}_0$ fluctuated with an estimate of $g_0$ including a single covariate and intercept. Smaller values are ordered first.</li>
<li><code>PartialCorrOrdering()</code> - Absolute value of partial correlation between the resudual $Y-\bar{Q}_n(A, W)$ and each covariate in <code>W</code>, conditional on $A$. Larger values are ordered first.</li>
<li><code>HDPSOrdering()</code> - Like the criterion for ordering covariates in the HDPS algorithm, not yet implemented.</li>
</ul>
<p>The <code>PreOrdered</code> search strategy has a second optional argument, <code>k_at_once</code>, which defaults to 1. At each step, the next <code>k_at_once</code> (or all available, if fewer than <code>k_at_once</code>) ordered covariates are added to the next estimate of $g_0$. Large <code>k_at_once</code> can speed up the procedure when there are many covariates, but it may be more prone to overfitting.</p>
<h2 id="cross-validation">Cross validation</h2>
<p>The number of steps to take while adding covariates to estimates of $g_0$ is determined by cross-validation. By default, <code>ctmle</code> uses 10-fold stratified cross-validation where folds are stratified by <code>A</code> if <code>Y</code> has more than 2 unique values, or combinations of <code>A</code> and <code>Y</code> otherwise.</p>
<p>General cross-validation schemes can be specified by passing the <code>cvplan</code> keyword argument an iterator of vectors of training set indexes. The <a href="https://github.com/JuliaStats/MLBase.jl">MLBase</a> package has some <a href="https://mlbasejl.readthedocs.org/en/latest/crossval.html#cross-validation-schemes">useful functions</a> to generate CV schemes.
In particular <code>StratifiedKfold</code> and <code>StratifiedRandomSub</code> are exported in TargetedLearning.jl.
<code>StratifiedRandomSub</code> is useful when you have a huge data set and want to save some time over doing K fold CV.</p>
<p>Even more generally, you can pass a vector of vectors of training indexes to <code>ctmle</code>, constructed however you like. If you want to run <code>ctmle</code> more than once with the same CV training sets, you need to do this, because the <code>StratifiedKfold</code> and <code>StratifiedRandomSub</code> iterators will produce new random indexes each time they are used. For example, <code>cvplan={training_idx_1, training_idx_2, training_idx_3}</code> where <code>training_idx_#</code> is a vector of integer indexes for a given training set.  You can also <code>collect</code> the <code>StratifiedKfold</code> or <code>StratifiedRandomSub</code> iterators to get a vector of vectors of indexes, and call <code>ctmle</code> with that multiple times. (<code>mycvplan=collect(StratifiedKFold(A, 10))</code>)</p>
<p>Finally, the <code>patience</code> keyword can be used to specify how long CV should continue after finding a local minimum. For example suppose <code>patience</code> is <code>5</code> and after 3 steps a minimum value of the CV loss is found. If it does not improve in another <code>5</code> steps, CV will will stop, choosing 3 steps. This can save quite a bit of time when there are many covariates.</p>
<h2 id="example-using-targetedlearningjl_1">Example using TargetedLearning.jl</h2>
<p>Continuing the <a href="../julia/#example">example</a> with the Lalonde data set, we now demonstrate how to estimate the ATE using CTMLE.
<a href="http://nbviewer.ipython.org/url/lendle.github.io/TargetedLearning.jl/user-guide/lalonde_example.ipynb#CTMLE-for-the-average-treatment-effect">The example can be found here.</a></p>
<div class="footnote">
<hr />
<ol>
<li id="fn:ctmle_paper">
<p><a href="http://www.degruyter.com/view/j/ijb.2010.6.1/ijb.2010.6.1.1181/ijb.2010.6.1.1181.xml">van der Laan, M. &amp; Gruber, S. (2010). Collaborative Double Robust Targeted Maximum Likelihood Estimation. <em>The International Journal of Biostatistics</em>, 6(1), pp. -. Retrieved 14 Apr. 2015, from doi:10.2202/1557-4679.1181</a>&#160;<a class="footnote-backref" href="#fnref:ctmle_paper" rev="footnote" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
<li id="fn:iv">
<p>An instrumental variable is a variable that affects only treatment but not outcome. Adjusting for an instrumental variable does not reduce bias in an estimate of $\psi_0$, and may induce a violation of the positivity assumption, which can lead to increased variance in the final estimate.&#160;<a class="footnote-backref" href="#fnref:iv" rev="footnote" title="Jump back to footnote 2 in the text">&#8617;</a></p>
</li>
</ol>
</div></div>
        </div>

        

        <script src="../../js/jquery-1.10.2.min.js"></script>
        <script src="../../js/bootstrap-3.0.3.min.js"></script>
        <script src="../../js/highlight.pack.js"></script>
        <script src="../../js/base.js"></script>
    </body>
</html>