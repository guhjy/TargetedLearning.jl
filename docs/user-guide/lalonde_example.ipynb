{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Lalonde example\n",
    "\n",
    "Throughout the user's guide, we'll use the [Lalonde dataset](http://users.nber.org/~rdehejia/nswdata2.html), in particular the \"Dehejia-Wahha Sample\" on that web page. It is originally from the paper\n",
    "Robert Lalonde, \"Evaluating the Econometric Evaluations of Training Programs,\" *American Economic Review*, Vol. 76, pp. 604-620. 1986.\n",
    "\n",
    "The output in this example was generated by converting an IJulia notebook to Markdown for the user's guide. This doesn't look great, but you can find a [better looking version rendered by nbviewer](http://nbviewer.ipython.org/url/lendle.github.io/TargetedLearning.jl/user-guide/lalonde_example.ipynb).\n",
    "\n",
    "#Getting the data set\n",
    "\n",
    "To download the example data set, run\n",
    "\n",
    "```julia\n",
    "include(joinpath(Pkg.dir(\"TargetedLearning\"), \"examples\", \"fetchdata.jl\"))\n",
    "fetchdata(ds=\"lalonde_dw\")\n",
    "```\n",
    "\n",
    "This only needs to be run once.\n",
    "\n",
    "# Loading the data set\n",
    "\n",
    "## Using readcsv\n",
    "\n",
    "The simplest way to read the data set is with `readcsv`. The CSV file we have has a header, so we'll pass `header=true`, and we'll get back a tuple containing a matrix with the numeric data in it (`readcsv` can figure out that it's all `Float64`s automatically) and a matrix with one row containing column names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1x10 Array{String,2}:\n",
       " \"treatment\"  \"age\"  \"education\"  …  \"nodegree\"  \"RE74\"  \"RE75\"  \"RE78\""
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dsfname = joinpath(Pkg.dir(\"TargetedLearning\"), \"examples\", \"data\", \"lalonde_dw.csv\")\n",
    "\n",
    "dat, colnames = readcsv(dsfname, header=true)\n",
    "\n",
    "#lets inspect colnames\n",
    "colnames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10-element Array{String,1}:\n",
       " \"treatment\"\n",
       " \"age\"      \n",
       " \"education\"\n",
       " \"black\"    \n",
       " \"hispanic\" \n",
       " \"married\"  \n",
       " \"nodegree\" \n",
       " \"RE74\"     \n",
       " \"RE75\"     \n",
       " \"RE78\"     "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#convert colnames to a vector instead of a matrix with one row\n",
    "colnames = reshape(colnames, size(colnames, 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`treatment` is obviously the treatment variable. The outcome variable is `RE78` (earnings in 1978), and `RE74` and `RE75` are earnings values prior to treatment. The others are potential baseline confounders. Check the link above for more information about the dataset.  We want to slice the matrix `dat` up to extract the treatment and outcome variable. Julia uses square brackets for indexing. The first dimension of a matrix is rows and the second is columns like R. If you want everything in one dimension, you put `:` (where as in R you can leave that dimension empty). You can index with booleans, integers, vectors of integers, ranges and some other things. Here are some examples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "445-element Array{Float64,1}:\n",
       " 1.0\n",
       " 1.0\n",
       " 1.0\n",
       " 1.0\n",
       " 1.0\n",
       " 1.0\n",
       " 1.0\n",
       " 1.0\n",
       " 1.0\n",
       " 1.0\n",
       " 1.0\n",
       " 1.0\n",
       " 1.0\n",
       " ⋮  \n",
       " 0.0\n",
       " 0.0\n",
       " 0.0\n",
       " 0.0\n",
       " 0.0\n",
       " 0.0\n",
       " 0.0\n",
       " 0.0\n",
       " 0.0\n",
       " 0.0\n",
       " 0.0\n",
       " 0.0"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#we know column 1 is treatment, so get it directly, and ask for all rows with :\n",
    "treatment = dat[:, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#suppose instead we want to find the position in colnames that has the value\n",
    "#\"treatment\", but we don't know it's the first one. There are a couple of ways\n",
    "#to do that.\n",
    "#(.== and operators starting with `.` in general indicate that we want to do\n",
    "#   an element wise operation)\n",
    "\n",
    "#(the ; at the end of the line suppresses output)\n",
    "treatment_take2 = dat[:, colnames .== \"treatment\", 1];"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "445-element Array{Float64,1}:\n",
       "     0.0 \n",
       "     0.0 \n",
       "     0.0 \n",
       "     0.0 \n",
       "     0.0 \n",
       "     0.0 \n",
       "     0.0 \n",
       "     0.0 \n",
       "     0.0 \n",
       "     0.0 \n",
       "     0.0 \n",
       "     0.0 \n",
       "     0.0 \n",
       "     ⋮   \n",
       "  9160.69\n",
       "  9210.45\n",
       "  9311.94\n",
       "  9319.44\n",
       " 10033.9 \n",
       " 10598.7 \n",
       " 10857.2 \n",
       " 12357.2 \n",
       " 13371.3 \n",
       " 16341.2 \n",
       " 16946.6 \n",
       " 23032.0 "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#the last column is the outcome so we can use the keyword `end`\n",
    "outcome = dat[:, end]\n",
    "\n",
    "#we can also use `end` in arithmetic, e.g.\n",
    "outcome_prebaseline = dat[:, end-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "445x8 Array{Float64,2}:\n",
       " 37.0  11.0  1.0  0.0  1.0  1.0      0.0        0.0 \n",
       " 22.0   9.0  0.0  1.0  0.0  1.0      0.0        0.0 \n",
       " 30.0  12.0  1.0  0.0  0.0  0.0      0.0        0.0 \n",
       " 27.0  11.0  1.0  0.0  0.0  1.0      0.0        0.0 \n",
       " 33.0   8.0  1.0  0.0  0.0  1.0      0.0        0.0 \n",
       " 22.0   9.0  1.0  0.0  0.0  1.0      0.0        0.0 \n",
       " 23.0  12.0  1.0  0.0  0.0  0.0      0.0        0.0 \n",
       " 32.0  11.0  1.0  0.0  0.0  1.0      0.0        0.0 \n",
       " 22.0  16.0  1.0  0.0  0.0  0.0      0.0        0.0 \n",
       " 33.0  12.0  0.0  0.0  1.0  0.0      0.0        0.0 \n",
       " 19.0   9.0  1.0  0.0  0.0  1.0      0.0        0.0 \n",
       " 21.0  13.0  1.0  0.0  0.0  0.0      0.0        0.0 \n",
       " 18.0   8.0  1.0  0.0  0.0  1.0      0.0        0.0 \n",
       "  ⋮                         ⋮                       \n",
       " 29.0   9.0  1.0  0.0  0.0  1.0   9268.94    9160.69\n",
       " 28.0   9.0  1.0  0.0  1.0  1.0  10222.4     9210.45\n",
       " 30.0  11.0  1.0  0.0  1.0  1.0      0.0     9311.94\n",
       " 25.0  10.0  1.0  0.0  1.0  1.0  13520.0     9319.44\n",
       " 28.0  11.0  1.0  0.0  1.0  1.0    824.389  10033.9 \n",
       " 22.0  10.0  0.0  0.0  0.0  1.0  27864.4    10598.7 \n",
       " 44.0   9.0  1.0  0.0  1.0  1.0  12260.8    10857.2 \n",
       " 21.0   9.0  1.0  0.0  0.0  1.0  31886.4    12357.2 \n",
       " 28.0  11.0  1.0  0.0  0.0  1.0  17491.5    13371.3 \n",
       " 29.0   9.0  0.0  1.0  0.0  1.0   9594.31   16341.2 \n",
       " 25.0   9.0  1.0  0.0  1.0  1.0  24731.6    16946.6 \n",
       " 22.0  10.0  0.0  0.0  1.0  1.0  25720.9    23032.0 "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#baseline covariates are everything between the first and last column, so we could use a range:\n",
    "baseline_covars = dat[:, 2:end-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using DataFrame\n",
    "\n",
    "The `readtable` function returns a `DataFrame` which allows for indexing by column names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10-element Array{Symbol,1}:\n",
       " :treatment\n",
       " :age      \n",
       " :education\n",
       " :black    \n",
       " :hispanic \n",
       " :married  \n",
       " :nodegree \n",
       " :RE74     \n",
       " :RE75     \n",
       " :RE78     "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "using DataFrames #`using` is like `library` in R\n",
    "\n",
    "dsfname = joinpath(Pkg.dir(\"TargetedLearning\"), \"examples\", \"data\", \"lalonde_dw.csv\")\n",
    "\n",
    "df = readtable(dsfname)\n",
    "\n",
    "#check the column names:\n",
    "names(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data frames in Julia are indexed by symbols instead of strings. Symbols are no entirely unlike strings, and are created in julia with `:symbolname` or `Symbol(\"symbolname\")`.\n",
    "\n",
    "Now let's get the treatment and outcome variables out of `df`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "445-element DataArray{Float64,1}:\n",
       "  9930.05\n",
       "  3595.89\n",
       " 24909.5 \n",
       "  7506.15\n",
       "   289.79\n",
       "  4056.49\n",
       "     0.0 \n",
       "  8472.16\n",
       "  2164.02\n",
       " 12418.1 \n",
       "  8173.91\n",
       " 17094.6 \n",
       "     0.0 \n",
       "     ⋮   \n",
       "     0.0 \n",
       "  1239.84\n",
       "  3982.8 \n",
       "     0.0 \n",
       "     0.0 \n",
       "  7094.92\n",
       " 12359.3 \n",
       "     0.0 \n",
       "     0.0 \n",
       " 16900.3 \n",
       "  7343.96\n",
       "  5448.8 "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "treatment = df[:treatment]\n",
    "outcome = df[:RE78]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that variables in a data frame are actually `DataArray`s instead of regular Julia `Array`s. The functions in TargetedLearning.jl currently only work with `Arrays` of floating point numbers, so we'll convert them.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array{Float64,1}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "treatment = convert(Array{Float64}, treatment)\n",
    "outcome = convert(Array{Float64}, outcome)\n",
    "typeof(outcome)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also index into the data frame using ranges, just like regular matrixes (but we only index on columns). Let's get the baseline covariates. When you get more than one column out of a data frame, you get back another data frame. For some reason that I do not know, `convert` won't work for us here, but `array` will get us what we need (a Julia array instead of a DataFrame)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "445x8 Array{Real,2}:\n",
       " 37  11  1  0  1  1      0.0        0.0 \n",
       " 22   9  0  1  0  1      0.0        0.0 \n",
       " 30  12  1  0  0  0      0.0        0.0 \n",
       " 27  11  1  0  0  1      0.0        0.0 \n",
       " 33   8  1  0  0  1      0.0        0.0 \n",
       " 22   9  1  0  0  1      0.0        0.0 \n",
       " 23  12  1  0  0  0      0.0        0.0 \n",
       " 32  11  1  0  0  1      0.0        0.0 \n",
       " 22  16  1  0  0  0      0.0        0.0 \n",
       " 33  12  0  0  1  0      0.0        0.0 \n",
       " 19   9  1  0  0  1      0.0        0.0 \n",
       " 21  13  1  0  0  0      0.0        0.0 \n",
       " 18   8  1  0  0  1      0.0        0.0 \n",
       "  ⋮               ⋮                     \n",
       " 29   9  1  0  0  1   9268.94    9160.69\n",
       " 28   9  1  0  1  1  10222.4     9210.45\n",
       " 30  11  1  0  1  1      0.0     9311.94\n",
       " 25  10  1  0  1  1  13520.0     9319.44\n",
       " 28  11  1  0  1  1    824.389  10033.9 \n",
       " 22  10  0  0  0  1  27864.4    10598.7 \n",
       " 44   9  1  0  1  1  12260.8    10857.2 \n",
       " 21   9  1  0  0  1  31886.4    12357.2 \n",
       " 28  11  1  0  0  1  17491.5    13371.3 \n",
       " 29   9  0  1  0  1   9594.31   16341.2 \n",
       " 25   9  1  0  1  1  24731.6    16946.6 \n",
       " 22  10  0  0  1  1  25720.9    23032.0 "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "baseline_covars = array(df[2:end-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Formulas\n",
    "\n",
    "One nice thing about DataFrames.jl is that is has support for formulas. They are similar to R's formulas, but there are some differences. Some packages, like GLM.jl take data frames with formulas as input. Those packages can be used for computing initial estimates, but TargetedLearning.jl does not support formulas and DataFrames directly.  However, you can use DataFrames.jl's functionality to take a DataFrame and a formula and get a numeric design matrix based on a formula.\n",
    "\n",
    "For example, suppose we'd like to include an age squared term and an interaction term between education and marital status. It looks like polynomial terms aren't implemented currently, so you'll have to manually make those terms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Formula: treatment ~ age + age2 + education + black + hispanic + married + nodegree + RE74 + RE75 + married & nodegree"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#create age squared\n",
    "df[:age2] = df[:age] .* df[:age];\n",
    "\n",
    "#if you want to suppress the intercept, use with -1\n",
    "# by default the first column will be a column of 1s, which we want.\n",
    "fm = treatment ~ age + age2 + education + black + hispanic + married + nodegree + RE74 + RE75 + married&nodegree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "445x11 Array{Float64,2}:\n",
       " 1.0  37.0  1369.0  11.0  1.0  0.0  1.0  1.0      0.0        0.0   1.0\n",
       " 1.0  22.0   484.0   9.0  0.0  1.0  0.0  1.0      0.0        0.0   0.0\n",
       " 1.0  30.0   900.0  12.0  1.0  0.0  0.0  0.0      0.0        0.0   0.0\n",
       " 1.0  27.0   729.0  11.0  1.0  0.0  0.0  1.0      0.0        0.0   0.0\n",
       " 1.0  33.0  1089.0   8.0  1.0  0.0  0.0  1.0      0.0        0.0   0.0\n",
       " 1.0  22.0   484.0   9.0  1.0  0.0  0.0  1.0      0.0        0.0   0.0\n",
       " 1.0  23.0   529.0  12.0  1.0  0.0  0.0  0.0      0.0        0.0   0.0\n",
       " 1.0  32.0  1024.0  11.0  1.0  0.0  0.0  1.0      0.0        0.0   0.0\n",
       " 1.0  22.0   484.0  16.0  1.0  0.0  0.0  0.0      0.0        0.0   0.0\n",
       " 1.0  33.0  1089.0  12.0  0.0  0.0  1.0  0.0      0.0        0.0   0.0\n",
       " 1.0  19.0   361.0   9.0  1.0  0.0  0.0  1.0      0.0        0.0   0.0\n",
       " 1.0  21.0   441.0  13.0  1.0  0.0  0.0  0.0      0.0        0.0   0.0\n",
       " 1.0  18.0   324.0   8.0  1.0  0.0  0.0  1.0      0.0        0.0   0.0\n",
       " ⋮                             ⋮                                   ⋮  \n",
       " 1.0  29.0   841.0   9.0  1.0  0.0  0.0  1.0   9268.94    9160.69  0.0\n",
       " 1.0  28.0   784.0   9.0  1.0  0.0  1.0  1.0  10222.4     9210.45  1.0\n",
       " 1.0  30.0   900.0  11.0  1.0  0.0  1.0  1.0      0.0     9311.94  1.0\n",
       " 1.0  25.0   625.0  10.0  1.0  0.0  1.0  1.0  13520.0     9319.44  1.0\n",
       " 1.0  28.0   784.0  11.0  1.0  0.0  1.0  1.0    824.389  10033.9   1.0\n",
       " 1.0  22.0   484.0  10.0  0.0  0.0  0.0  1.0  27864.4    10598.7   0.0\n",
       " 1.0  44.0  1936.0   9.0  1.0  0.0  1.0  1.0  12260.8    10857.2   1.0\n",
       " 1.0  21.0   441.0   9.0  1.0  0.0  0.0  1.0  31886.4    12357.2   0.0\n",
       " 1.0  28.0   784.0  11.0  1.0  0.0  0.0  1.0  17491.5    13371.3   0.0\n",
       " 1.0  29.0   841.0   9.0  0.0  1.0  0.0  1.0   9594.31   16341.2   0.0\n",
       " 1.0  25.0   625.0   9.0  1.0  0.0  1.0  1.0  24731.6    16946.6   1.0\n",
       " 1.0  22.0   484.0  10.0  0.0  0.0  1.0  1.0  25720.9    23032.0   1.0"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#take the field named `m` from the created ModelMatrix object\n",
    "expanded_baseline_covars = ModelMatrix(ModelFrame(fm, df)).m"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's clunky, but will get the job done. More detailed documentation is [here](http://dataframesjl.readthedocs.org/en/latest/formulas.html).\n",
    "\n",
    "### Missing data and categorical data\n",
    "\n",
    "[DataFrames.jl](http://dataframesjl.readthedocs.org/en/latest/) and [DataArrays.jl](https://github.com/JuliaStats/DataArrays.jl) have ways of handling both missing data and categorical data. TargetedLearning.jl does not, so you'll have to deal with those issues ahead of time. The documentation for those packages has more information on both.\n",
    "\n",
    "# TMLE for the average treatment effect\n",
    "\n",
    "To use TargetedLearning.jl, we first need to scale the outcome to be between 0 and 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.0,1.0)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scaled_outcome = (outcome .- minimum(outcome)) ./ (maximum(outcome) - minimum(outcome))\n",
    "extrema(scaled_outcome)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now need to estimate $\\bar{Q}_0$ and $g_0$.  We'll use logistic regression for both, regressing on the columns created using the formula [above](#Formulas)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: could not import Base.add! into NumericExtensions\n"
     ]
    }
   ],
   "source": [
    "using TargetedLearning\n",
    "\n",
    "n = size(expanded_baseline_covars, 1)\n",
    "\n",
    "#estimate \\bar{Q} by regressing on baseline covars and treatment\n",
    "Qfit = lreg([expanded_baseline_covars treatment], scaled_outcome)\n",
    "\n",
    "#computed estimates of logit(\\bar{Q}_n(a,W) for a=1 and 0\n",
    "#linpred returns the linear part of the prediction, which is in the logit scale for logistic regression\n",
    "logitQnA1 = linpred(Qfit, [expanded_baseline_covars ones(n)])\n",
    "logitQnA0 = linpred(Qfit, [expanded_baseline_covars zeros(n)])\n",
    "\n",
    "#esitmate g by regressing on W\n",
    "gfit = lreg(expanded_baseline_covars, treatment)\n",
    "#compute estimated probabilities of treatment\n",
    "gn1 = predict(gfit, expanded_baseline_covars);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given initial estimates $\\bar{Q}_n$ and $g_n$, we just need to call the `tmle` function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Estimate\n",
       "      Estimate Std. Error Lower 95% CL Upper 95% CL\n",
       "ATE  0.0271417  0.0112603   0.00507195    0.0492115\n"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scaled_estimate = tmle(logitQnA1, logitQnA0, gn1, treatment, scaled_outcome, param=ATE(), weightedfluc=false)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `tmle` function returns an estimate along with an estimated variance based on the EIC. It's important to note that this variance is asymptotically consistent if both $\\bar{Q}_n$ and $g_n$ are consistent, and is conservative if $g_n$ is consistent but $\\bar{Q}_n$ is not. If $g_n$ is not consistent, then EIC based variance estimate is not reliable.\n",
    "\n",
    "The outcome in the original data set represented earnings, but this estimate is for the ATE of an outcome scaled between 0 and 1. To get it back to the original scale, we can just multiply by the scaling factor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Estimate\n",
       "     Estimate Std. Error Lower 95% CL Upper 95% CL\n",
       "psi   1636.86    679.085      305.879      2967.84\n"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "estimate_orig_scale = scaled_estimate * (maximum(outcome) - minimum(outcome))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After transforming the original estimate, the estimand name is now called \"psi\" instead of \"ATE\". To make the output look nicer, we can rename it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Estimate\n",
       "                 Estimate Std. Error Lower 95% CL Upper 95% CL\n",
       "ATE on earnings   1636.86    679.085      305.879      2967.84\n"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "name!(estimate_orig_scale, \"ATE on earnings\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# CTMLE for the average treatment effect\n",
    "\n",
    "To use CTMLE to estimate the ATE, we'll use the same initial estimate of $\\mbox{logit}\\bar{Q}_n$ as above, stored in `logitQnA1` and `logitQnA0`.  We'll also use the same covariates created by [expanding](#Formulas) the original variables in the data set via a formula, with a first column of ones.\n",
    "\n",
    "First, we create a CV plan which we can use for multiple calls to `ctmle`, so results don't change based on different CV folds.  By default, the CV plan is generated inside the `ctmle` function, and can be different if you don't reset the seed random seed before each run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "srand(20) #set the random seed\n",
    "\n",
    "#collect the treatment indexes from the StratifiedKfold object. \n",
    "cvplan = collect(StratifiedKfold(treatment, 10)) #10 fold cv stratified by treatment;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Estimate\n",
       "     Estimate Std. Error Lower 95% CL Upper 95% CL\n",
       "ATE  0.027733  0.0108727   0.00642292    0.0490431\n"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ctmle_est = ctmle(logitQnA1, logitQnA0, expanded_baseline_covars, treatment, scaled_outcome, cvplan=cvplan)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To see what covariates were added in fluctuations, call `flucinfo` on the estimate:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Fluctuation info:\n",
       "Covariates added in 1 steps to 1 fluctuations.\n",
       "Fluc 1 covars added: [1]\n"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "flucinfo(ctmle_est)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case, there is only one fluctuation, and $g_0$ is only estimated using the intercept. As above, we can rescale the estimate back to the original scale. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Estimate\n",
       "     Estimate Std. Error Lower 95% CL Upper 95% CL\n",
       "psi   1636.86    679.085      305.879      2967.84\n"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scaled_estimate * (maximum(outcome) - minimum(outcome))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see what happens with an unadjusted initial $\\bar{Q}_n$. Here we'll just use the mean of the outcome by treatment. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Estimate\n",
       "      Estimate Std. Error Lower 95% CL Upper 95% CL\n",
       "ATE  0.0275032  0.0115604   0.00484526    0.0501612\n"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unadjusted_logitQnA1 = fill(logit(mean(scaled_outcome[treatment.==1])), n)\n",
    "unadjusted_logitQnA0 = fill(logit(mean(scaled_outcome[treatment.==0])), n)\n",
    "unadjusted_ctmle_est = ctmle(unadjusted_logitQnA1,\n",
    "                            unadjusted_logitQnA0,\n",
    "                            expanded_baseline_covars,\n",
    "                            treatment,\n",
    "                            scaled_outcome,\n",
    "                            cvplan=cvplan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Fluctuation info:\n",
       "Covariates added in 10 steps to 3 fluctuations.\n",
       "Fluc 1 covars added: [1,8,10,2,11]\n",
       "Fluc 2 covars added: [6,4,9,7]\n",
       "Fluc 3 covars added: [5]\n"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "flucinfo(unadjusted_ctmle_est)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So here we see three fluctuations, with a total of 10 covariates including the intercept being used to estimate $g_0$ out of 11 possible.  The estimate is nearly the same as the previous one, with a slightly larger standard error. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 0.3.7-pre",
   "language": "julia",
   "name": "julia 0.3"
  },
  "language_info": {
   "name": "julia",
   "version": "0.3.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
